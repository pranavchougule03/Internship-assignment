# -*- coding: utf-8 -*-
"""submission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vRgadOufWUeN3CdK6LbRCBUse53wAEFJ
"""

# pip install duckduckgo-search openai==0.28 beautifulsoup4 cohere tiktoken langchain torch

from duckduckgo_search import DDGS
def find_url(query):
    results = []
    with DDGS() as ddgs:
        results = [r for r in ddgs.text(query, max_results=2)]
    results = [r['href'] for r in results]
    return results

import openai
def generate_google_search_query(question, api_key):
    openai.api_key = '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'

    # Define the system prompt for the OpenAI API
    system_prompt = f"""Write 5 different google search queries to search online based on following query: {question}\n"""

    # Generate a completion using the OpenAI API
    response = openai.Completion.create(
        engine="gpt-3.5-turbo-instruct",
        prompt=system_prompt,
        temperature=0.5,
        max_tokens=150,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0
    )

    # Extract the generated search query from the API response
    search_query = response.choices[0].text.strip()
    search_query = search_query.split('\n')
    return search_query

import requests
from bs4 import BeautifulSoup

def scrape_webpage_text(url):
    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code != 200:
        return None

    # Parse the HTML content of the response
    soup = BeautifulSoup(response.content, 'html.parser')

    # Extract all the text from the webpage, including text from any HTML tag
    text_content = ''
    for tag in soup(['script', 'style']):
        tag.decompose()
    for tag in soup(['html', 'body', 'head', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li', 'div']):
        for text in tag.stripped_strings:
            text_content += text + ' '

    # Return the text content as a string
    return text_content

from transformers import pipeline
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Load the summarization pipeline
summarizer = pipeline("summarization", model="Falconsai/text_summarization",
                      tokenizer="Falconsai/text_summarization",
                     device = 1)

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=5000,
    chunk_overlap=100,  # Adjust overlap to avoid context discontinuity
    is_separator_regex=False  # Use word boundaries for better splitting
)

# Function to process text sequentially and generate summaries
def generate_summary_sequential(text):
    if text == None: return ""
    chunks = text_splitter.split_text(text)
    summaries = []
    for chunk in chunks:
      summary = summarizer(chunk, max_length=150, min_length=30, do_sample=False)[0]['summary_text']
      summaries.append(summary)
    return "\n".join(summaries)

import requests
import openai
import langchain

# Replace with your GPT API key
openai.api_key = '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'


def gpt3_query(chunk, query):
    try:
        response = openai.Completion.create(
            engine="gpt-3.5-turbo-instruct",
            prompt=f"Based on the following information:\n{chunk}\n\n give me some information about:\n{query} use only the given data generate answer dont add any irrelevant data in the answer.",
#             max_tokens=200,  # Adjust response length as needed
            temperature=0.7,  # Adjust temperature for desired creativity
            stop=None,
            n=1,
            stream=False
        )
        return response.choices[0].text
    except openai.error.OpenAIError as e:
        print(f"GPT-3 error: {e}")
        return ""  # Handle errors appropriately (e.g., retry, log, return default)


user_queries = ["Identify the industry in which Canoo operates, along with its size, growth rate, trends, and key players.",
                "Identify key trends in the market, including changes in consumer behavior, technological advancements, and shifts in the competitive landscape.",
                "Analyze Canoo's main competitors, including their market share, products or services offered, pricing strategies, and marketing efforts.",
                "Gather information on Canoo's financial performance, including its revenue, profit margins, return oninvestment, and expense structure."]

def append_text_to_file(text, file_path):
    with open(file_path, 'a') as file:
        file.write(text + '\n')

import re
my_api_key = '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'

for user_query in user_queries:
  search_queries = generate_google_search_query(user_query,my_api_key)
  append_text_to_file(user_query + "\n\n\n",'new_report1.txt')
  for search_query in search_queries:
    search_query = re.sub(r'^\d+', '', search_query)
    search_query = re.sub(r'[^\w\s\']', '', search_query)
    urls = find_url(search_query)
    append_text_to_file(search_query + "\n",'new_report1.txt')
    for url in urls:
      scraped_text = scrape_webpage_text(url)
      context_summary = generate_summary_sequential(scraped_text)
      report = gpt3_query(context_summary,search_query)
      append_text_to_file(report,'new_report1.txt')
  append_text_to_file("\n\n\n\n",'new_report1.txt')

